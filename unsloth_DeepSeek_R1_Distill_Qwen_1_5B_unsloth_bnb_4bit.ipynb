{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYaudnUaoHCQj1rZzUcO0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athulskrish/unsloth-DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit/blob/main/unsloth_DeepSeek_R1_Distill_Qwen_1_5B_unsloth_bnb_4bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit/tree/main"
      ],
      "metadata": {
        "id": "BO1sCj3c-tR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nigpik6Z-m7-"
      },
      "outputs": [],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit"
      ],
      "metadata": {
        "id": "JY097KaM-yb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth[colab-new] --quiet\n",
        "!pip install transformers accelerate bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "rScXv1uq-0tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import gc"
      ],
      "metadata": {
        "id": "y0qH-aSQPxGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4qvILf4vP2Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load the model\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n"
      ],
      "metadata": {
        "id": "ahH3OLU7QBwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n"
      ],
      "metadata": {
        "id": "XWNx9deXQD-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Set up FastLanguageModel for inference\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "8eGl68-xQJES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Step 6: Create a text streamer for real-time output\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)"
      ],
      "metadata": {
        "id": "vLnHiuDsQNCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define a function to generate responses\n",
        "def generate_response(prompt, max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate a response using the DeepSeek-R1-Distill model\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Input text prompt\n",
        "        max_new_tokens (int): Maximum number of tokens to generate\n",
        "        temperature (float): Sampling temperature (0.0 = deterministic, 1.0 = random)\n",
        "        top_p (float): Nucleus sampling threshold\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length-max_new_tokens\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            streamer=text_streamer,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the original prompt from the response\n",
        "    response = response[len(prompt):].strip()\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "G0p4B_udQSKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Example usage\n",
        "print(\"Model loaded successfully! Here's a test:\")\n",
        "print(\"=\" * 50)\n"
      ],
      "metadata": {
        "id": "7Ueb9UquQVrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompt\n",
        "test_prompt = \"What is the capital of France?\"\n",
        "\n",
        "print(f\"Question: {test_prompt}\")\n",
        "print(\"Answer: \", end=\"\")\n"
      ],
      "metadata": {
        "id": "sx-CGc4zQYja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and display response\n",
        "response = generate_response(test_prompt, max_new_tokens=256)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Model is ready for use!\")\n"
      ],
      "metadata": {
        "id": "ASIt8Jn4QfD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Interactive chat function\n",
        "def chat():\n",
        "    \"\"\"\n",
        "    Interactive chat function\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting interactive chat. Type 'quit' to exit.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input:\n",
        "            print(\"Assistant: \", end=\"\")\n",
        "            response = generate_response(user_input, max_new_tokens=512)\n",
        "            print()  # New line after response\n",
        "chat()\n"
      ],
      "metadata": {
        "id": "LS0GvHo-QiQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Advanced generation function with custom parameters\n",
        "def advanced_generate(prompt, max_new_tokens=512, temperature=0.7, top_p=0.9,\n",
        "                     top_k=50, repetition_penalty=1.1):\n",
        "    \"\"\"\n",
        "    Advanced generation with more parameters\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length-max_new_tokens\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()\n",
        "advanced_generate(\"what is the speed of light\")"
      ],
      "metadata": {
        "id": "5LscCGnzQmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTo80vbjQqUj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}